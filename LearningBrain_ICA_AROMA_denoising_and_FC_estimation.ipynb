{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denoising fMRI data and functional connectivity estimation\n",
    "================================================\n",
    "\n",
    "This is an example script for denoising and FC estimation from fMRI (1 subject) on data preprocessed in fmriprep with `--use-aroma` flag.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Importing packages\n",
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "%matplotlib inline\n",
    "\n",
    "from os import listdir\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np # the Python array package\n",
    "import matplotlib.pyplot as plt  # the Python plotting package\n",
    "\n",
    "from sklearn import preprocessing # for normalization of confounds columns\n",
    "\n",
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "from nilearn import input_data\n",
    "from nilearn import signal\n",
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "import nibabel as nib\n",
    "\n",
    "\n",
    "from denoise import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Creating atlas\n",
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Power ROIs coordinates\n",
    "\n",
    "power = datasets.fetch_coords_power_2011()\n",
    "power_coords = np.vstack((power.rois['x'], power.rois['y'], power.rois['z'])).T\n",
    "\n",
    "# Creating masker file\n",
    "\n",
    "power_spheres = input_data.NiftiSpheresMasker(\n",
    "    seeds = power_coords, \n",
    "    #smoothing_fwhm = 6, \n",
    "    radius = 5,\n",
    "    detrend = True, \n",
    "    standardize = True,\n",
    "    low_pass = 0.08, \n",
    "    high_pass = 0.009,\n",
    "    t_r = 2\n",
    ")\n",
    "\n",
    "parcellation = power_spheres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Loading data\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading functional data\n",
    "top_dir = '/home/finc/Downloads/fmriprep/'\n",
    "out_dir = '/home/finc/Downloads/fmriprep/'\n",
    "\n",
    "sess = ['ses-1', 'ses-2', 'ses-3', 'ses-4']\n",
    "tasks = ['dualnback']\n",
    "subs = listdir(top_dir)\n",
    "\n",
    "#suffix = 'space-MNI152NLin2009cAsym_preproc.nii.gz'\n",
    "suffix = 'space-MNI152NLin2009cAsym_variant-smoothAROMAnonaggr_preproc.nii.gz'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Obtaining timeseries from ROIs\n",
    "---------------------------\n",
    "Creating 5D matrix with mean timeseries within each ROI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = listdir(top_dir)\n",
    "\n",
    "timeseries_all = np.zeros((len(subs), len(sess), len(tasks), 340, 264))\n",
    "\n",
    "for sub in range(len(subs)):\n",
    "    for ses in range(len(sess)):\n",
    "        for task in range(len(tasks)):\n",
    "            \n",
    "            sub_dir = top_dir + subs[sub] + '/' + sess[ses] + '/func/'\n",
    "            data = sub_dir + subs[sub] + '_' + sess[ses] + '_task-' + tasks[task] + '_bold_' + suffix\n",
    "            \n",
    "            # Loading confound data\n",
    "            confounds_path = sub_dir + subs[sub] + '_' + sess[ses] + '_task-' + tasks[task] + '_bold_confounds.tsv'\n",
    "            confounds = pd.read_csv(confounds_path, delimiter = '\\t')\n",
    "\n",
    "            # Select columns of interest\n",
    "            confounds_anat = confounds[confounds.filter(regex='CSF|WhiteMatter').columns]\n",
    "            \n",
    "            # Standardize confounds\n",
    "            confounds_anat = standardize(confounds_anat)\n",
    "\n",
    "            # Add scrubbing columns\n",
    "            confounds_clean = confounds_anat\n",
    "\n",
    "            # Save preprocessed confound file\n",
    "            confounds_clean.to_csv(sub_dir + 'confounds_' + tasks[task] + '_clean_aroma.csv', sep = ',', index = False)\n",
    "            confounds_clean_path = sub_dir + 'confounds_' + tasks[task] + '_clean_aroma.csv'\n",
    "                     \n",
    "            timeseries = parcellation.fit_transform(data, confounds = confounds_clean_path)\n",
    "            \n",
    "            timeseries_all[sub, ses, task, :, :] = timeseries\n",
    "            \n",
    "np.save(out_dir + 'LearningBrain_all_timeseries_AROMA_power.npy', timeseries_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Creating task conditions vectors\n",
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onsets_1back = np.array([4, 72, 140, 208, 276, 344, 412, 480, 548, 616])\n",
    "onsets_2back = np.array([38, 106, 174, 242, 310, 378, 446, 514, 582, 650])\n",
    "vol_num = 340\n",
    "\n",
    "TR = 2\n",
    "duration = 30\n",
    "\n",
    "vector_n_back = np.zeros((vol_num, 3))\n",
    "\n",
    "for i in range(len(vector_n_back)):\n",
    "    if i in onsets_1back/TR:\n",
    "        for k in range(int(duration/TR)):\n",
    "            vector_n_back[i + k, 0] = 1\n",
    "    if i in onsets_2back/TR:\n",
    "        for k in range(int(duration/TR)):\n",
    "            vector_n_back[i + k, 1] = 1\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "            \n",
    "for i in range(len(vector_n_back)):\n",
    "    if vector_n_back[i,0] == 0 and vector_n_back[i,1] == 0:\n",
    "        vector_n_back[i,2] = 1\n",
    "        \n",
    "dual1back = vector_n_back[:,0].astype(bool)\n",
    "dual2back = vector_n_back[:,1].astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Obtaining corelation matrices\n",
    "---------------------------\n",
    "Creating 5D matrix with static corelation matrices for each task condition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlarion_matrices_dual = np.zeros((len(subs), len(sess), 2, len(timeseries_all[0, 0, 0, 0, : ]), len(timeseries_all[0, 0, 0, 0, : ])))\n",
    "\n",
    "\n",
    "for sub in range(len(timeseries_all[:, 0, 0, 0, 0])):\n",
    "    for ses in range(len(timeseries_all[0,:,0,0,0])):                 \n",
    "        timeseries_dual1back = timeseries_all[sub, ses, 0, dual1back, :]\n",
    "        timeseries_dual2back = timeseries_all[sub, ses, 0, dual2back, :]\n",
    "        \n",
    "        correlation_measure = ConnectivityMeasure(kind = 'correlation')\n",
    "        fc1 = correlation_measure.fit_transform([timeseries_dual1back])[0]\n",
    "        np.fill_diagonal(fc1, 0)\n",
    "        \n",
    "        fc2 = correlation_measure.fit_transform([timeseries_dual2back])[0]\n",
    "        np.fill_diagonal(fc2, 0)\n",
    "                     \n",
    "        correlarion_matrices_dual[sub, ses, 0, :, :] = fc1\n",
    "        correlarion_matrices_dual[sub, ses, 1, :, :] = fc2\n",
    "\n",
    "\n",
    "np.save(out_dir + 'LearningBrain_matrices_dual_aCompCor_power.npy', correlarion_matrices_dual)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Plotting\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-back\n",
    "\n",
    "plotting.plot_matrix(correlarion_matrices_dual[1, 1, 0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-back\n",
    "\n",
    "plotting.plot_matrix(correlarion_matrices_dual[1, 1, 1, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2424770882376519"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeseries_power_filtered[1,1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
